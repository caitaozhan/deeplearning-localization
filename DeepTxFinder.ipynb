{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import seaborn as sns\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "from skimage import io, transform\n",
    "from torch import tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "sys.path.insert(0, '/home/krishna/deeplearning-localization')\n",
    "from input_output import Default\n",
    "from utility import Utility\n",
    "from torch._six import container_abcs, string_classes, int_classes\n",
    "import re\n",
    "np_str_obj_array_pattern = re.compile(r'[SaUO]')\n",
    "default_collate_err_msg_format = (\n",
    "    \"default_collate: batch must contain tensors, numpy arrays, numbers, \"\n",
    "    \"dicts or lists; found {}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Common modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize:\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, noise_floor):\n",
    "        self.noise_floor = noise_floor\n",
    "\n",
    "    def __call__(self, matrix):\n",
    "        matrix -= self.noise_floor\n",
    "        max_value = max([max(l) for l in matrix])\n",
    "        matrix /= (max_value)\n",
    "        return matrix.astype(np.float32)\n",
    "\n",
    "class Metrics:\n",
    "    '''Evaluation metrics\n",
    "    '''\n",
    "\n",
    "    @staticmethod\n",
    "    def localization_error_regression(pred_batch, truth_batch, debug=False):\n",
    "        '''euclidian error when modeling the output representation is a matrix (image)\n",
    "           both pred and truth are batches, typically a batch of 32\n",
    "        '''\n",
    "        error = []\n",
    "        for pred, truth in zip(pred_batch, truth_batch):\n",
    "            error_row = []\n",
    "            for i in range(0, len(pred), 2):\n",
    "                one = pred[i:i+2] - truth[i:i+2]   # one pair of predicted location and ground truth\n",
    "                error_row.append(np.sqrt(np.dot(one, one)))\n",
    "            error.append(np.mean(error_row))\n",
    "            if debug:\n",
    "                print('pred', pred, 'truth', truth, 'err', error)\n",
    "        return error\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss(pred, y):\n",
    "        n = len(pred) * len(pred[0])\n",
    "        summ = np.sum((pred - y)**2)\n",
    "        return summ/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_padding(batch, max_len):\n",
    "    \"\"\"add zeros to elements that are not maximum length\"\"\"\n",
    "    for i in range(len(batch)):\n",
    "        diff = max_len - len(batch[i])\n",
    "        if diff > 0:                      # padding\n",
    "            zeros = torch.zeros(diff)\n",
    "            padded = torch.cat((batch[i], zeros), 0)\n",
    "            batch[i] = padded\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
    "    elem = batch[0]\n",
    "    elem_type = type(elem)\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        max_len = len(max(batch, key=len))\n",
    "        min_len = len(min(batch, key=len))\n",
    "        if max_len != min_len:\n",
    "            my_padding(batch, max_len)\n",
    "        out = None\n",
    "        if torch.utils.data.get_worker_info() is not None:\n",
    "            # If we're in a background process, concatenate directly into a\n",
    "            # shared memory tensor to avoid an extra copy\n",
    "            numel = sum([x.numel() for x in batch])\n",
    "            storage = elem.storage()._new_shared(numel)\n",
    "            out = elem.new(storage)\n",
    "        return torch.stack(batch, 0, out=out)\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
    "            # array of string classes and object\n",
    "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
    "                raise TypeError(default_collate_err_msg_format.format(elem.dtype))\n",
    "\n",
    "            return my_collate([torch.as_tensor(b) for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int_classes):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, string_classes):\n",
    "        return batch\n",
    "    elif isinstance(elem, container_abcs.Mapping):\n",
    "        return {key: my_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n",
    "        return elem_type(*(my_collate(samples) for samples in zip(*batch)))\n",
    "    elif isinstance(elem, container_abcs.Sequence):\n",
    "        # check to make sure that the elements in batch have consistent size\n",
    "        it = iter(batch)\n",
    "        elem_size = len(next(it))\n",
    "        if not all(len(elem) == elem_size for elem in it):\n",
    "            raise RuntimeError('each element in list of batch should be of equal size')\n",
    "        transposed = zip(*batch)\n",
    "        return [my_collate(samples) for samples in transposed]\n",
    "\n",
    "    raise TypeError(default_collate_err_msg_format.format(elem_type))\n",
    "\n",
    "\n",
    "def my_uncollate(y_num, y_float):\n",
    "    \"\"\"this is for uncollating the target_float\"\"\"\n",
    "    y_float_tmp = []\n",
    "    for ntx, y_f in zip(y_num, y_float):\n",
    "        y_float_tmp.append(y_f[:ntx*2])\n",
    "    return np.array(y_float_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SensorInputDatasetRegression(Dataset):\n",
    "    '''Sensor reading input dataset -- for multi TX\n",
    "       Output is image, model as a image segmentation problem\n",
    "    '''\n",
    "    def __init__(self, root_dir: str, grid_len:int, transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "            root_dir:  directory with all the images\n",
    "            labels:    labels of images\n",
    "            transform: optional transform to be applied on a sample\n",
    "        '''\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.length = len(os.listdir(self.root_dir))\n",
    "        self.sample_per_label = self.get_sample_per_label()\n",
    "        self.grid_len = grid_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length * self.sample_per_label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder = int(idx/self.sample_per_label)\n",
    "        folder = format(folder, '06d')\n",
    "        matrix_name = str(idx%self.sample_per_label) + '.npy'\n",
    "        matrix_path = os.path.join(self.root_dir, folder, matrix_name)\n",
    "        target_name = str(idx%self.sample_per_label) + '.target.npy'\n",
    "        target_arr = self.get_regression_target(folder, target_name)\n",
    "        target_num = int(len(target_arr)/2)\n",
    "        matrix = np.load(matrix_path)\n",
    "        if self.transform:\n",
    "            matrix = self.transform(matrix)\n",
    "        target_arr = self.min_max_normalize(target_arr)\n",
    "        sample = {'matrix': matrix, 'target': target_arr, 'target_num': target_num, 'index': idx}\n",
    "        return sample\n",
    "\n",
    "    def get_sample_per_label(self):\n",
    "        folder = glob.glob(os.path.join(self.root_dir, '*'))[0]\n",
    "        samples = glob.glob(os.path.join(folder, '*.npy'))\n",
    "        targets = glob.glob(os.path.join(folder, '*.target.npy'))\n",
    "        return len(samples) - len(targets)\n",
    "\n",
    "    def get_regression_target(self, folder: str, target_name: str):\n",
    "        '''\n",
    "        Args:\n",
    "            folder: example of folder is 000001\n",
    "        Return:\n",
    "            a two dimension matrix\n",
    "        '''\n",
    "        target_file = os.path.join(self.root_dir, folder, target_name)\n",
    "        target = np.load(target_file)\n",
    "        target = np.reshape(target, -1)\n",
    "        return target.astype(np.float32)\n",
    "\n",
    "    def min_max_normalize(self, target_arr: np.ndarray):\n",
    "        '''scale the localization to a range of (0, 1)\n",
    "        '''\n",
    "        target_arr /= Default.grid_length\n",
    "        return target_arr\n",
    "\n",
    "    def undo_normalize(self, arr: np.ndarray):\n",
    "        arr *= self.grid_len\n",
    "        return arr\n",
    "\n",
    "\n",
    "tf = T.Compose([\n",
    "     Normalize(Default.noise_floor_ipsn),\n",
    "     T.ToTensor()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix shape: torch.Size([1, 100, 100])\n",
      "matrix shape: tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.4099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])\n",
      "target: [0.0591504  0.0522483  0.47112995 0.09088928 0.29469407 0.4888554\n",
      " 0.54737383 0.5435013 ]\n",
      "length: 25000\n",
      "---\n",
      "\n",
      "matrix type: torch.float32\n",
      "target type: float32\n",
      "target num: 8\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "i = 0\n",
    "root_dir = 'ipsn_testbed/train_random'\n",
    "# root_dir = 'data/305train'\n",
    "sensor_input_dataset = SensorInputDatasetRegression(root_dir=root_dir, grid_len=Default.grid_length, transform=tf)\n",
    "sensor_input_dataloader = DataLoader(sensor_input_dataset, batch_size=32, shuffle=True, num_workers=1, collate_fn=my_collate)\n",
    "print('matrix shape:', sensor_input_dataset[i]['matrix'].shape)\n",
    "print('matrix shape:', sensor_input_dataset[i]['matrix'])\n",
    "print('target:', sensor_input_dataset[i]['target'])\n",
    "print('length:', sensor_input_dataset.__len__())\n",
    "\n",
    "print('---\\n')\n",
    "# testing\n",
    "root_dir = 'ipsn_testbed/test_random'\n",
    "# root_dir = 'data/305test'\n",
    "sensor_input_test_dataset = SensorInputDatasetRegression(root_dir=root_dir, grid_len=Default.grid_length, transform=tf)\n",
    "sensor_input_test_dataloader = DataLoader(sensor_input_test_dataset, batch_size=32, shuffle=True, num_workers=1, collate_fn=my_collate)\n",
    "print('matrix type:', sensor_input_test_dataset[i]['matrix'].dtype)\n",
    "print('target type:', sensor_input_test_dataset[i]['target'].dtype)\n",
    "print('target num:', sensor_input_test_dataset[i]['target_num'])\n",
    "print(sensor_input_test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_NoTx(nn.Module):\n",
    "    \"\"\"this CNN predicts # of TX \"\"\"\n",
    "    \n",
    "    def __init__(self, max_ntx):\n",
    "        super(CNN_NoTx, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3))\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3))\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=(3, 3))\n",
    "        self.flat = nn.Flatten()\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.dense = nn.Linear(8192, 512)\n",
    "        self.dense1 = nn.Linear(512, max_ntx)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "#         x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.flat(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.dense(x))\n",
    "        x = self.dense1(x)\n",
    "#         x = self.softmax(self.dense1(x))\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "numtx = CNN_NoTx(max_ntx=10)\n",
    "print(numtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(pred, y):\n",
    "    '''\n",
    "    Args:\n",
    "        pred: Tensor -- (batch size, num tx * 2)\n",
    "        y   : Tensor -- (batch size, num tx * 2)\n",
    "    '''\n",
    "    batch_size = len(pred)\n",
    "    dimension  = len(pred[0])\n",
    "    pred_m, y_m = torch.zeros((batch_size, dimension)), torch.zeros((batch_size, dimension))\n",
    "    for i in range(batch_size):\n",
    "        pred2, y2 = [], []   # for one sample\n",
    "        for j in range(0, dimension, 2):\n",
    "            pred2.append(pred[i][j:j+2])\n",
    "            y2.append(y[i][j:j+2])\n",
    "        pred2, y2 = match_helper(pred2, y2)\n",
    "        pred_m[i] = pred2\n",
    "        y_m[i]    = y2\n",
    "    return pred_m, y_m\n",
    "\n",
    "def match_helper(pred, y):\n",
    "    ''' up to now, assume len(pred) and len(y) is equal\n",
    "        do a matching of the predictions and truth\n",
    "    Args:\n",
    "        pred: list<Tensor<float>> -- pairs of locations\n",
    "        y   : list<Tensor> -- pairs of locations\n",
    "    Return:\n",
    "        Tensor<float>, Tensor<float>\n",
    "    '''\n",
    "    distances = np.zeros((len(pred), len(y)))\n",
    "    for i in range(len(pred)):\n",
    "        for j in range(len(y)):\n",
    "            distances[i, j] = Utility.distance(pred[i], y[j])\n",
    "\n",
    "    matches = []\n",
    "    k = 0\n",
    "    while k < min(len(pred), len(y)):\n",
    "        min_error = np.min(distances)\n",
    "        min_error_index = np.argmin(distances)\n",
    "        i = min_error_index // len(y)\n",
    "        j = min_error_index % len(y)\n",
    "        matches.append((i, j, min_error))\n",
    "        distances[i, :] = np.inf\n",
    "        distances[:, j] = np.inf\n",
    "        k += 1\n",
    "\n",
    "    pred_m, y_m = [], []\n",
    "    for i, j, e in matches:\n",
    "        pred_m.append(pred[i])\n",
    "        y_m.append(y[j])\n",
    "\n",
    "    return torch.cat(pred_m), torch.cat(y_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_i(nn.Module):\n",
    "\n",
    "    def __init__(self, max_ntx):\n",
    "        super(CNN_i, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3))\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3))\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=(3, 3))\n",
    "        self.flat = nn.Flatten()\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.dense = nn.Linear(8192, 512)\n",
    "        self.dense1 = nn.Linear(512,2*max_ntx)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.flat(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.dense(x))\n",
    "        y1 = self.dense1(x)\n",
    "        return y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_input_train_cnn2_dataset = []\n",
    "sensor_input_train_cnn2_dataloader = []\n",
    "data1 = ['cnn2_train1', 'cnn2_train2', 'cnn2_train3', 'cnn2_train4', 'cnn2_train5', 'cnn2_train6', 'cnn2_train7', 'cnn2_train8', 'cnn2_train9', 'cnn2_train10']\n",
    "for d in range(len(data1)):\n",
    "#     root_dir = './data3/' + data1[d]\n",
    "    root_dir = 'ipsn_testbed/dtxf_random/' + data1[d]\n",
    "    sensor_input_dataset = SensorInputDatasetRegression(root_dir = root_dir, grid_len=Default.grid_length, transform = tf)\n",
    "    sensor_input_train_cnn2_dataset.append(sensor_input_dataset)\n",
    "    sensor_input_dataloader = DataLoader(sensor_input_dataset, batch_size=32, shuffle=True, num_workers=3, collate_fn=my_collate)\n",
    "    sensor_input_train_cnn2_dataloader.append(sensor_input_dataloader)\n",
    "\n",
    "sensor_input_test_cnn2_dataset = []\n",
    "sensor_input_test_cnn2_dataloader = []\n",
    "data2 = ['cnn2_test1', 'cnn2_test2', 'cnn2_test3', 'cnn2_test4', 'cnn2_test5', 'cnn2_test6', 'cnn2_test7', 'cnn2_test8', 'cnn2_test9', 'cnn2_test10']\n",
    "for d in range(len(data2)):\n",
    "#     root_dir = './data3/' + data2[d]\n",
    "    root_dir = 'ipsn_testbed/dtxf_random/' + data2[d]\n",
    "    sensor_input_test_dataset = SensorInputDatasetRegression(root_dir = root_dir, grid_len=Default.grid_length, transform = tf)\n",
    "    sensor_input_test_cnn2_dataset.append(sensor_input_test_dataset)\n",
    "    sensor_input_test_dataloader = DataLoader(sensor_input_test_dataset, batch_size=32, shuffle=True, num_workers=3, collate_fn=my_collate)\n",
    "    sensor_input_test_cnn2_dataloader.append(sensor_input_test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "max_ntx = 10\n",
    "models = []\n",
    "optimizers = []\n",
    "schedulers = []\n",
    "path = 'model_dtxf/3.1-cnn2-ipsn_{}.pt'\n",
    "for i in range(1, max_ntx+1):\n",
    "    net    = CNN_i(i)\n",
    "    device = torch.device('cuda')\n",
    "    model  = net.to(device)\n",
    "    models.append(model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizers.append(optimizer)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.1)\n",
    "    schedulers.append(scheduler)\n",
    "\n",
    "for i in range(len(sensor_input_train_cnn2_dataloader)):\n",
    "    if i <= 4 or i >= 8:\n",
    "        continue\n",
    "    model1_minloss = 1000\n",
    "\n",
    "    sensor_input_dataset         = sensor_input_train_cnn2_dataset[i]\n",
    "    sensor_input_test_dataset    = sensor_input_test_cnn2_dataset[i]\n",
    "    sensor_input_dataloader      = sensor_input_train_cnn2_dataloader[i]\n",
    "    sensor_input_test_dataloader = sensor_input_test_cnn2_dataloader[i]\n",
    "    model = models[i]\n",
    "    optimizer = optimizers[i]\n",
    "    scheduler = schedulers[i]\n",
    "\n",
    "    criterion = nn.MSELoss()  # criterion is the loss function\n",
    "    num_tx = i + 1\n",
    "    train_losses2_epoch = []\n",
    "    train_errors2_epoch = []\n",
    "    test_losses2_epoch  = []\n",
    "    test_errors2_epoch  = []\n",
    "    print_every = 20\n",
    "    error_every = 20\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'epoch = {epoch}')\n",
    "        train_losses = []\n",
    "        train_errors = []\n",
    "        test_losses  = []\n",
    "        test_errors  = []\n",
    "        model.train()\n",
    "        for t, sample in enumerate(sensor_input_dataloader):\n",
    "            X = sample['matrix'].to(device)\n",
    "            y = sample['target'].to(device)\n",
    "            y_num   = int(sample['target_num'][0])\n",
    "            if y_num != num_tx:\n",
    "                raise(f\"number of TX wrong! num_tx={num_tx}, y_num={y_num}\")\n",
    "\n",
    "            pred = model(X)\n",
    "\n",
    "            pred, y = match(pred, y)\n",
    "\n",
    "            loss = criterion(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            y    = y.cpu().detach().numpy()\n",
    "            pred = sensor_input_dataset.undo_normalize(pred)\n",
    "            y    = sensor_input_dataset.undo_normalize(y)\n",
    "            if t % print_every == 0:\n",
    "                train_errors.extend(Metrics.localization_error_regression(pred, y))\n",
    "                print(f't = {t}, loss = {loss.item()}; error= {np.mean(train_errors[-32:])}')\n",
    "\n",
    "        model.eval()\n",
    "        for t, sample in enumerate(sensor_input_test_dataloader):\n",
    "\n",
    "            X = sample['matrix'].to(device)\n",
    "            y = sample['target'].to(device)\n",
    "            y_num   = int(sample['target_num'][0])\n",
    "            if y_num != num_tx:\n",
    "                raise(f\"number of TX wrong! num_tx={num_tx}, y_num={y_num}\")\n",
    "\n",
    "            pred = model(X)\n",
    "            pred, y = match(pred, y)\n",
    "\n",
    "            loss = criterion(pred, y)\n",
    "            test_losses.append(loss.item())\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            y    = y.cpu().detach().numpy()\n",
    "            pred = sensor_input_test_dataset.undo_normalize(pred)\n",
    "            y    = sensor_input_test_dataset.undo_normalize(y)\n",
    "            if t % print_every == 0:\n",
    "                test_errors.extend(Metrics.localization_error_regression(pred, y))\n",
    "                print(f't = {t}, loss = {loss.item()}; error= {np.mean(test_errors[-32:])}')\n",
    "\n",
    "\n",
    "        if np.mean(test_losses) < model1_minloss:\n",
    "            torch.save(model.state_dict(), path.format(num_tx))\n",
    "            model1_minloss = np.mean(test_losses)\n",
    "\n",
    "        scheduler.step()\n",
    "        print('train loss of num of TX   =', np.mean(train_losses))\n",
    "        print('test  loss of num of TX   =', np.mean(test_losses))\n",
    "        print('train error of num of TX  =', np.mean(train_errors))\n",
    "        print('test error of num of TX   =', np.mean(test_errors))\n",
    "        train_losses2_epoch.append((np.mean(train_losses)))\n",
    "        train_errors2_epoch.append((np.mean(train_errors)))\n",
    "        test_losses2_epoch.append((np.mean(test_losses)))\n",
    "        test_errors2_epoch.append((np.mean(test_errors)))\n",
    "        clear_output(True)\n",
    "        print(f'num tx = {num_tx}')\n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title('Loss', fontsize=30)\n",
    "        plt.xlabel('Epoch', fontsize=30)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        plt.plot(train_losses2_epoch, label='Train', linewidth=5)\n",
    "        plt.plot(test_losses2_epoch, label='Test', linewidth=5)\n",
    "        plt.legend(fontsize=30)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title('Error', fontsize=30)\n",
    "        plt.xlabel('Epoch', fontsize=30)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        plt.plot(train_errors2_epoch, label='Train', linewidth=5)\n",
    "        plt.plot(test_errors2_epoch, label='Test', linewidth=5)\n",
    "        plt.legend(fontsize=30)\n",
    "        plt.tight_layout()\n",
    "        if epoch == num_epochs - 1:\n",
    "            plt.savefig(f'model_dtxf/3.1-cnn2_{num_tx}.png')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
